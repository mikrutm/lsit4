{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "model_args = {}\n",
    "# we will use batch size of 64 in Stochastic Gradient Descent (SGD) optimization of the network\n",
    "model_args['batch_size'] = 64 \n",
    "# learning rate is how fast it will descend\n",
    "model_args['lr'] = .07\n",
    "# the number of epochs is the number of times you go through the full dataset\n",
    "model_args['epochs'] = 20\n",
    "# L2 (ridge) penalty\n",
    "model_args['L2_lambda'] = 5.e-3\n",
    "# L1 (LASSO) penalty\n",
    "model_args['L1_lambda'] = 2.e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428e5f274f9b43daaf516cc80c1ec0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "!rm -r ./data\n",
    "# normalize dataset\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean, std)])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# we divide this data into training and validation subsets\n",
    "train_subset, validation_subset = torch.utils.data.random_split(cifar10_train, [40000, 10000])\n",
    "test_subset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# subsample to speedup training (colab has notebook lifetime limit)\n",
    "train_subset = torch.utils.data.Subset(train_subset, range(20000))\n",
    "validation_subset = torch.utils.data.Subset(validation_subset, range(5000))\n",
    "test_subset = torch.utils.data.Subset(test_subset, range(5000))\n",
    "\n",
    "# define dataloaders\n",
    "loader_kwargs = {'batch_size': model_args['batch_size'], \n",
    "                 'num_workers': 2, \n",
    "                 'pin_memory': True, \n",
    "                 'shuffle': True}\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, **loader_kwargs)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_subset, **loader_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, **loader_kwargs)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    '''\n",
    "    simple CNN model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "vgg_cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    'vgg22': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 256, 'M', 512, 512, 512, 512, 512, 'M', 512, 512, 512, 512, 512, 'M']\n",
    "}\n",
    "\n",
    "\n",
    "def train_with_regularization(model, device, train_loader, optimizer, criterion, \n",
    "                              epoch_number, \n",
    "                              L1_lambda, \n",
    "                              L2_lambda):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    # get subsequent batches over the data in a given epoch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send data tensors to GPU (or CPU)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # this will zero out the gradients for this batch\n",
    "        optimizer.zero_grad()\n",
    "        # this will execute the forward() function\n",
    "        output = model(data)\n",
    "        # calculate loss using c\n",
    "        loss = criterion(output, target)\n",
    "        # L2 regularization implemeted by hand\n",
    "        L2_norm = sum( (p**2).sum() for p in model.parameters())\n",
    "        # L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        #\n",
    "        loss_regularized = loss + L1_norm*L1_lambda + L2_norm*L2_lambda\n",
    "        # backpropagate the loss\n",
    "        loss_regularized.backward()\n",
    "        # update the model weights (with assumed learning rate)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    #print('Train Epoch: {}'.format(epoch_number))\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    #print('\\tTrain set: Average loss: {:.4f}'.format(train_loss))\n",
    "    return train_loss\n",
    "    \n",
    "def test(model, device, test_loader, criterion, message=False):\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    # this is just inference, we don't need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device) \n",
    "            output = model(data)\n",
    "            # calculate and sum up batch loss\n",
    "            test_loss += criterion(output, target) \n",
    "            # get the index of class with the max probability \n",
    "            prediction = output.argmax(dim=1)  \n",
    "            #_, predicted = torch.max(outputs.data, axis=1)\n",
    "            # item() returns value of the given tensor\n",
    "            correct += prediction.eq(target).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    #if message is not None:\n",
    "    #    print('\\t{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    #        message, test_loss, correct, len(test_loader.dataset), 100.*accuracy))\n",
    "    return test_loss.cpu(), accuracy\n",
    "\n",
    "def run_training(model, device, criterion, optimizer, no_epochs,\n",
    "                 L1_lambda=0., \n",
    "                 L2_lambda=0.):\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    validation_accuracy = []\n",
    "    test_accuracy = []\n",
    "    for epoch_number in range(1, no_epochs+1):\n",
    "        train_loss.append(train_with_regularization(model, device, train_loader, \n",
    "                                optimizer, criterion, epoch_number,\n",
    "                                L1_lambda, L2_lambda))\n",
    "        val_loss, val_acc = test(model, device, validation_loader, \n",
    "                                criterion, 'Validation set')\n",
    "        validation_loss.append(val_loss)\n",
    "        validation_accuracy.append(val_acc)\n",
    "        # we also collect test accuracies for every epoch\n",
    "        _, test_acc = test(model, device, test_loader, criterion)\n",
    "        test_accuracy.append(test_acc)\n",
    "    # and select test accuracy for the best epoch (with the highest validation accuracy)\n",
    "    best_accuracy = test_accuracy[np.argmax(validation_accuracy)]\n",
    "    return train_loss, validation_loss, best_accuracy\n",
    "\n",
    "def plot_loss(train_loss, validation_loss, title):\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"subsequent epochs\")\n",
    "    plt.ylabel('average loss')\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
    "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=model_args['lr'],\n",
    "                      weight_decay=0.)\n",
    "\n",
    "train_loss_0, val_loss_0, best_accuracy = run_training(model, device,criterion, optimizer, model_args['epochs'], L1_lambda= 0.5,L2_lambda= 0.01)\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy))\n",
    "plot_loss(train_loss_0, val_loss_0, 'Without any regularization')\n",
    "model_0_params = [p.detach().cpu().numpy() for p in model.parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
